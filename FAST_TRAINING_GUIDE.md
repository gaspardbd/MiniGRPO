# Guide d'EntraÃ®nement Rapide avec LoRA

## ProblÃ¨me rÃ©solu


(`train_fast.py`): **10x plus rapide**
- LoRA : seulement ~1% des paramÃ¨tres entraÃ®nÃ©s
- Quantization 4-bit optionnelle
- Pas de reference model sÃ©parÃ© (utilise les log probs actuels)
- OptimisÃ© comme votre TP avec GRPOTrainer

## ğŸ“Š Comparaison avec votre TP

| Aspect | Votre TP (GRPOTrainer) | train_fast.py |
|--------|----------------------|---------------|
| LoRA | âœ… Oui | âœ… Oui |
| Quantization | âœ… 4-bit | âœ… 4-bit (optionnel) |
| Reference model | Implicite | Pas de modÃ¨le sÃ©parÃ© |
| ImplÃ©mentation | HuggingFace TRL | Custom (votre code) |
| Rewards | 2 fonctions sÃ©parÃ©es | 1 fonction combinÃ©e |
| ContrÃ´le | LimitÃ© | Total |

## Utilisation rapide

### 1. Dans Colab

```python
%cd /content/MiniGRPO
!git pull origin main

# Installer PEFT si nÃ©cessaire
!pip install peft bitsandbytes

# Lancer l'entraÃ®nement rapide
!python train_fast.py
```

### 2. Localement

```bash
cd MiniGRPO
pip install peft bitsandbytes
python train_fast.py
```

## âš™ï¸ Configuration par dÃ©faut

```python
# ModÃ¨le
model_name = "Qwen/Qwen2.5-0.5B-Instruct"
use_lora = True
use_quant = False  # Mettez True si problÃ¨me de mÃ©moire

# Dataset
num_prompts = 500  # Au lieu de 100k
batch_size = 8     # AugmentÃ© de 4 Ã  8
num_rollout = 2    # RÃ©duit de 4 Ã  2

# Training
max_steps = 100
learning_rate = 1e-5
clip_epsilon = 0.28  # Comme votre TP
kl_weight = 0.0      # Beta = 0 comme votre TP

# LoRA
lora_r = 64
lora_alpha = 128
lora_dropout = 0.1
```

## ğŸ“ˆ Vitesse attendue

| Configuration | Temps par step | 100 steps |
|--------------|----------------|-----------|
| train.py (original) | ~9 min | ~15h |
| train_fast.py (LoRA) | ~1 min | ~1h40 |
| train_fast.py (LoRA + quant) | ~45 sec | ~1h15 |

## ğŸ’¾ Checkpoints et Ã©valuation

### Sauvegardes

Le script sauvegarde :
- `output_fast/step_25/` - Checkpoint Ã  25 steps
- `output_fast/step_50/` - Checkpoint Ã  50 steps
- `output_fast/step_75/` - Checkpoint Ã  75 steps
- `output_fast/final/` - LoRA adapters finaux
- `output_fast/final_merged/` - ModÃ¨le mergÃ© (LoRA + base)

### Ã‰valuation

```bash
# Ã‰valuer sur tout le test set GSM8K
python evaluate.py --model_path output_fast/final_merged

# Ã‰valuer sur 100 samples seulement
python evaluate.py --model_path output_fast/final_merged --num_samples 100

# Ã‰valuer un checkpoint intermÃ©diaire
python evaluate.py --model_path output_fast/step_50
```

Le script d'Ã©valuation :
- âœ… DÃ©tecte automatiquement si c'est un checkpoint LoRA
- âœ… Merge les adapters automatiquement
- âœ… Utilise le mÃªme format de prompt que l'entraÃ®nement
- âœ… Calcule l'accuracy sur GSM8K
- âœ… Sauvegarde les rÃ©sultats en JSON

### Exemple de sortie d'Ã©valuation

```
============================================================
EVALUATION RESULTS
============================================================
Accuracy: 34.52%
Correct: 453/1319
============================================================

ğŸ“ Sample predictions:

Example 1:
  Question: Janet's ducks lay 16 eggs per day...
  True answer: 18
  Generated: 18
  Correct: âœ“
```

## ğŸ”§ Personnalisation

### Pour un training plus long

Modifiez dans `train_fast.py` :

```python
num_prompts = 2000   # Plus de donnÃ©es
max_steps = 500      # Plus de steps
```

### Pour plus de vitesse

```python
use_quant = True     # Active la quantization 4-bit
num_rollout = 1      # RÃ©duit Ã  1 rollout
batch_size = 16      # Augmente le batch
max_length = 200     # RÃ©duit la longueur max
```

### Pour Ã©viter OOM (Out of Memory)

```python
use_quant = True           # Active la quantization
batch_size = 4             # RÃ©duit le batch
num_rollout = 1            # RÃ©duit les rollouts
max_length = 150           # RÃ©duit la longueur
gradient_accumulation = 2  # Ajouter accumulation
```

## ğŸ› Troubleshooting

### Erreur: "CUDA out of memory"

```python
# Dans train_fast.py, modifiez:
use_quant = True
batch_size = 2
num_rollout = 1
```

### Erreur: "peft not found"

```bash
pip install peft bitsandbytes
```

### Rewards toujours Ã  0

Le modÃ¨le ne gÃ©nÃ¨re pas le bon format. Lancez :
```bash
python test_generation.py
```

### Loss explose (>100)

```python
# RÃ©duisez le learning rate
learning_rate = 5e-6  # Au lieu de 1e-5
```

## ğŸ“Š DiffÃ©rences avec train.py original

| Feature | train.py | train_fast.py |
|---------|----------|---------------|
| LoRA | âŒ Non | âœ… Oui |
| Reference model | âœ… ModÃ¨le sÃ©parÃ© sur CPU | âŒ Utilise log probs actuels |
| Vitesse | âŒ 9 min/step | âœ… 1 min/step |
| MÃ©moire | âŒ Haute | âœ… Basse |
| Dataset | 100k prompts | 500 prompts (configurable) |
| Batch size | 4 | 8 |
| Rollouts | 4 | 2 |

## ğŸ¯ Workflow complet

```bash
# 1. Test rapide du modÃ¨le
python test_generation.py

# 2. EntraÃ®nement rapide
python train_fast.py

# 3. Ã‰valuation
python evaluate.py --model_path output_fast/final_merged --num_samples 100

# 4. Si bon, Ã©valuation complÃ¨te
python evaluate.py --model_path output_fast/final_merged
```

## ğŸ“ Notes

- **LoRA** : EntraÃ®ne seulement des matrices de rang faible (~1% des paramÃ¨tres)
- **No ref model** : On utilise les log probs de la politique actuelle comme rÃ©fÃ©rence au lieu d'un modÃ¨le sÃ©parÃ© (simplifie et accÃ©lÃ¨re)
- **Quantization** : Optionnelle, rÃ©duit la mÃ©moire de 4x
- **num_prompts** : LimitÃ© Ã  500 par dÃ©faut pour la vitesse, augmentez selon vos besoins

Bon entraÃ®nement rapide ! ğŸš€

